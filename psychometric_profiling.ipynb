{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Elements of Psychometric Profiling\n",
    "This is a Jupyter notebook, that reimplements a few key elements of [Michal Kosinski's R tutorial](https://www.michalkosinski.com/data-mining-tutorial) for a workshop on Psychometric Profiling.\n",
    "\n",
    "You are going to\n",
    "0. Learn how to use a Jupyter notebook\n",
    "1. Have a look at the original user and likes data\n",
    "2. Transform the data into a format that is better suited for processing\n",
    "3. Clean the data\n",
    "4. Condense the data by reducing its dimensionality\n",
    "5. Train a model that can predict a user's personality by xer likes\n",
    "6. Evaluate the model\n",
    "\n",
    "Please, take some notes along the way about what strikes you most in this approach to Psychometric Profiling and what you would like to share with the other participants of the workshop.\n",
    "\n",
    "Especially if you have had no or little prior experience with programming before, some of the following advise may be useful\n",
    "- You are highly encouraged to try [pair programming](https://en.wikipedia.org/wiki/Pair_programming), a concept where two people program together. This enables rapid skill sharing, creativity and reduces coding errors. Please try to pair up, such that persons without coding experience always have a more experienced partner. Don't let one person write all the code and the other observe all the time, but take turns and discuss what you are doing!\n",
    "- Don't be afraid to try things out, you won't break anything.\n",
    "- If you are stuck, check the documentation, ask another person or your favorite search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Jupyter Notebook\n",
    "\n",
    "You are working in a [Jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/), which allows to write and run code step-by-step and interactively in the browser. The underlying programming language is called [Python](https://www.python.org/) and is installed on most operating systems.\n",
    "\n",
    "You can select a cell to write code. When you are done, you can run the code by clicking on the play arrow at the top of this window or press `ctrl-return`\n",
    "\n",
    "Try it out with the code cell below. You can try some of the following\n",
    "- Math: `3 * 2 + 1`\n",
    "- Text: `print(\"Hello,\" + \" world!\")`\n",
    "- Help: `help(print)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello,\" + \" world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "The data used here is part of [Michal Kosinski's tutorial](https://www.michalkosinski.com/data-mining-tutorial). It consists of three tables, saved in so called [CSV files](https://en.wikipedia.org/wiki/Comma-separated_values). The tables are:\n",
    "- user data including their ID, age, gender, political view and psychological profile according to an OCEAN/BigFive personality test\n",
    "- likes data which is an ID and the name of the like\n",
    "- users_likes data, which are user-ID and like-ID combinations, to see which user liked what\n",
    "\n",
    "Usefule Python code can be loaded from so called libraries, that contain methods for specific purposes. A prominent library for data analysis is called [Pandas](http://pandas.pydata.org/pandas-docs/stable/). The following code imports the method `read_csv` to read the data from [CSV files](https://en.wikipedia.org/wiki/Comma-separated_values). To call a method, you need to write `method_name(arguments)`, where here `arguments` is just the location of the specific CSV file.\n",
    "\n",
    "Pandas structures tabular data in [DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), that supply many useful methods in dot-notation. One of them is the `head` method, which can be called without any arguments to show the first five rows of a DataFrame. Run the cell below, to see the structure of the `users` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             userid  gender  age  political   ope   con   ext  \\\n",
      "0  54f34605aebd63f7680e37ffd299af79       0   33        0.0  1.26  1.65  1.17   \n",
      "1  86399f8c44ba54224b2e60177ca89fa9       1   35        0.0  1.07  0.17 -0.14   \n",
      "2  84fab50f3c60d1fdc83aa91b5e584a78       1   36        0.0  0.89  1.28  0.86   \n",
      "3  f3b8fdaccce12ef6352bfad4d6052fe9       0   39        NaN  0.33 -1.01 -0.33   \n",
      "4  8b06ea5e9cb87c61da387995450607f7       0   31        NaN  0.15  0.47  1.17   \n",
      "\n",
      "    agr   neu  \n",
      "0 -1.76  0.61  \n",
      "1  1.49  0.30  \n",
      "2  1.07  0.99  \n",
      "3 -0.68  0.92  \n",
      "4 -1.01 -0.32  \n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "users = read_csv('data/users.csv')\n",
    "users_likes = read_csv('data/users-likes.csv')\n",
    "likes = read_csv('data/likes.csv')\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see anything special in the data so far? For example the column `gender` is encoded as 0 or 1. Maybe you also wonder about the `NaN` in the `political` column? This is an abbreviation for \"Not a number\" and denotes a missing value, i.e. the user's political orientation is unknown. Otherwise political orientation is also encoded as 0 or 1 (0.0 is the floating point notation for the integer 0). The last five columns stand for the OCEAN profile of the given user.\n",
    "\n",
    "You can try out the `head` method on the other data. Or you might wonder how to show more than 5 rows. Try `help(users.head)`. Or you might wonder what other methods can be called on a `DataFrame`. You can see this simply with `help(users)`. For example the `describe` method can also be interesting for a first overview on unknown data. \n",
    "\n",
    "Another useful hint: You can use the `TAB` key for auto-completion, so writing `users.he` and pressing `TAB` should automatically write `head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "Up to now the data is structured into three separate tables. The tools that are used by Psychometric Profiling need the data in a single structure, which is called a sparse matrix and has the following form, where a 1 in a cell means that the user of that column liked the like of that row and a `.` denotes an empty cell, meaning that the given user did not like the given like\n",
    "\n",
    "|  .  | UserA | UserB | UserC | ... |\n",
    "|-----|-------|-------|-------|-----|\n",
    "|LikeA|   .   |   1   |   1   |  .  |\n",
    "|LikeB|   1   |   .   |   .   |  .  |\n",
    "|LikeC|   .   |   .   |   1   |  .  |\n",
    "| ... |   .   |   .   |   .   |  .  |\n",
    "\n",
    "Merging the tables into one table is possibly with Pandas' merge method. Once that is done, we can construct a sparse matrix from it. The implementation is done by the library [scipy](https://docs.scipy.org/doc/scipy/reference/sparse.html), which contains many methods for scientific programming.\n",
    "\n",
    "Reformatting data to fit the required format can be tedious. You need to know the right methods. But it usually is not very enlightening, so you do not have to spend to much time on the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import merge\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "def construct_user_likes_matrix(users, likes, users_likes):\n",
    "    users[\"user_row\"] = range(len(users))  # Adding a counter to the users\n",
    "    likes[\"like_row\"] = range(len(likes))  # Adding a counter to the likes\n",
    "    ul = merge(users_likes, likes[[\"likeid\", \"like_row\"]], on=\"likeid\")  # Adds the like counter to the users_likes table\n",
    "    ul = merge(ul, users[[\"userid\", \"user_row\"]], on=\"userid\")  # Adds the users counter to the users_likes table\n",
    "    ul_sparse = coo_matrix(([True] * len(ul), (ul[\"user_row\"], ul[\"like_row\"])),  # Required format for the constructor is (cell_values, (column_indices, row_indices))\n",
    "                           shape=(len(users), len(likes)),  # Users are in the columns, likes are in the rows\n",
    "                           dtype=bool)  # Every cell contains True or False / 1 or 0\n",
    "    return ul_sparse\n",
    "\n",
    "\n",
    "ul_sparse = construct_user_likes_matrix(users, likes, users_likes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Another tedious step is to clean the data. Typically, prediction models only work well when there is a lot of data. We already saw, that there is a lot of missing data. Kosinski et al propose a classical way: Throw out all the users that have not liked many likes and throw away all the likes that have not been liked by many users.\n",
    "\n",
    "Again, it is not too instructive to understand all the details of the code. Still, you can try what happens if you change the parameters `min_likes` or `min_users`. How does it change the size of the resulting matrix and users table? What other ways could be used to deal with the problem that prediction models dont work that well? What does this mean for the people being represented in the users data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_ul(ul_sparse, users, min_likes=150, min_users=50):\n",
    "    ul_sparse = ul_sparse.tocsc()\n",
    "    while True:\n",
    "        i = sum(ul_sparse.shape)\n",
    "        # sum returns a 1xLikes matrix, A1 returns as flattened 1D-array\n",
    "        enough_likes = ul_sparse.sum(axis=1).T.A1 > min_likes\n",
    "        ul_sparse = ul_sparse[enough_likes, :][:, ul_sparse.sum(axis=0).A1 > min_users]\n",
    "        users = users.iloc[enough_likes]\n",
    "        if i == sum(ul_sparse.shape):\n",
    "            break\n",
    "    return ul_sparse, users\n",
    "\n",
    "\n",
    "ul_sparse_trimmed, users_trimmed = trim_ul(ul_sparse, users)  # This trims both users and likes to about one 15th\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many likes fit the criterium, how many users are still present in the cleaned data? Can you find out how many likes a given user has? Or by how many users a like has been liked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "The goal of this psychometric profiling is to predict some part of a person's personality by xer likes in social network. There are very many likes to predict just one or a few numbers (like a person's openness). Many statistical models do not work well in a situation like this. One solution is to use a \"dimensionality reduction\". Here, this tries to find some sort of most varying combinations of likes in the data. For example, if most users liked \"Cats\", then liking \"Cats\" is not very informative and will be removed from the data. Instead, the algorithm tries to find combinations of likes, where the users disagree a lot. For example, half of the users liked \"Dogs\" and \"Sausages\", whereas the other half did not click these items.\n",
    "\n",
    "Why might this be useful?\n",
    "\n",
    "The library used for this is called scikit-learn and contains many methods for predictive statistics, a.k.a. machine learning. You can also try other techniques to reduce the dimension of the data, like other algorithms for [dimensionality reduction](https://scikit-learn.org/stable/modules/unsupervised_reduction.html) or [manifold learning](https://scikit-learn.org/stable/modules/manifold.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "ul_lowdim_svd = TruncatedSVD(30).fit_transform(ul_sparse_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "Now that we have clean data in a condensed form, we can use a classical algorithm called Linear Regression to train a model that shall predict a given output by the data. In our case, we want to predict the openness score by the condensed likes.\n",
    "\n",
    "You can try other algorithms, like [Generalized Linear Models](https://scikit-learn.org/stable/modules/linear_model.html#linear-model) or [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html) or [Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html).\n",
    "What part of reality is modeled by these algorithms? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ope_lm = LinearRegression().fit(ul_lowdim_svd, users_trimmed[\"ope\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a model\n",
    "How well are we performing with our model? To answer this question, we need to define some measure of quality. We might stick to statistical methods, like [Pearson's correlation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html). This method returns two values, the first one being a correlation between the data sets (0=no correlation, 1=perfect correlation) and the second a p-value, testing how likely two random data sets would produce such a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4439544378352614, 0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(ope_lm.predict(ul_lowdim_svd), users_trimmed[\"ope\"])  # First value should be above .4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caveat: The model we have trained has seen the data we are testing it on during training. Basically, we were telling it how high a user scored on openness and then ask it whether it could remember this value. We might be more interested in evaluating it on data that it did not see during training. The following code does this by a so called train-test-split. With the linear model on the condensed data the difference is negligible. But this would not be true for all models.\n",
    "\n",
    "Try to play around:\n",
    "- Use other models\n",
    "- Predict other values\n",
    "- Pick a different evaluation function\n",
    "- Clean or condense the data differently\n",
    "- ...\n",
    "\n",
    "If you want to keep some result, you can add a new cell below by clicking on the `+` on top of this window.\n",
    "\n",
    "So now you have learned more about the technical background. What does this tell us about Psychometric Profiling in our society? What are the assumptions in it? What world view does it convey? Who can use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4598477290191918, 1.1568807436605519e-44)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ul_train, ul_test, open_train, open_test = train_test_split(ul_lowdim_svd, users_trimmed[\"ope\"],\n",
    "                                                           test_size=0.1)\n",
    "ope_lm = LinearRegression().fit(ul_train, open_train)\n",
    "pearsonr(ope_lm.predict(ul_test), open_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
